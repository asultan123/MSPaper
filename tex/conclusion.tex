\chapter{Conclusion}
\label{chap:conclude}

In this thesis a Hybrid GEMM and Direct Convolution Accelerator (HERO) was
introduced. HERO is a neural network accelerator that maintains computation
generality by supporting matrix multiplication while retaining computational
efficiency when running the common case configurations of the convolution
operation. The design of HERO was derived from data aware design process. To
direct that process a (ConvolutIon statIstics GAtherer) \ac{CIGAR} tool was used
to find the common case of the convolution operation that needed to be support
by HERO directly. To find optimal configurations of HERO, a HERO accelerator
TEMPlate Optimizer (TEMPO) was introduced. TEMPO was driven by an analytical
model used to maximize on-chip PE utilization. Additionally, a novel descriptor
driven on-chip memory primitive capable of orchestrating energy efficient on
chip data movements within \ac{HERO} called SAM was presented. To compile arbitrary pytorch
models down to SAM descriptors a HERO layer compiler was discussed. To estimate
the performance and energy efficiency of arbitrary HERO configurations a cycle
accurate simulation platform driven by a SystemC simulation backend and a python
evaluation frontend was developed. Finally, an analysis of an optimal HERO
configuration when running 695 networks in the TIMM library was discussed.

HERO was found to perform well on a wide variety of network configurations. It
achieved a median FPS of ~91 FPS with a median speedup of 4.87X over CPU
baseline. The estimated bandwidth required for the configuration of HERO studied
was 19.65GiB which is within the PC4-21300 DDR4 specification. With that
configuration of DRAM the median inferences/J is 57. The total on-chip area is
estimated at 0.34 $mm^2$. 

In terms of utilization, the optimal configuration found by TEMPO enables some
networks to benefit substantially from HERO while others less so. Instances of
poor layer mapping to HERO include depthwise and group convolution layers that
were not considered when developing HERO as well as lowered layers that required more
compute resources than what was available in the configuration studied. 

To further the development of HERO explicit support of depthwise and grouped
convolution layers is necessary along with an exploration of alternative forms
of concurrency within HERO. Additionally, more convolution layer configurations
need to be supported directly in HERO, specifically convolution layers with a
stride size greater than 1. Along with increasing convolution support, support
for other layer types like Batch normalization layers and activation layers
needs to be included in HERO to minimize data movement between the CPU and HERO.
Similarly, lowering and lifting transformation required to extend support to
arbitrary convolution layers needs to be incorporated into HERO to minimize
off-chip data movement. Finally a larger scale exploration of different HERO
architectures using the developed simulation platform is required to validate
and refine TEMPO's analytical model.