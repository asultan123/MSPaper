\chapter{Network Compilation}
\label{chap:net_compile}

\section{Layer Equivalence}

Accelerator generality can be maintained by supporting general matrix
multiplication operations as well as convolution operations common in many
modern deep neural networks. Convolution operations can be converted into
general matrix multiplication through the use of lowering and lifting techniques
like Im2col discussed in \cite{cafe_con_troll}. To support matrix multiplication
within a convolution accelerator we can reporpose existing compute and memory
hardware on chip to perform both GEMM and convolutions operations or we can
establish a mathematical equivelence between GEMM operations and Conv by
reinterpret GEMM into a special case of convolution, namely convolutions with a
1x1 kernel size. This avoids the overhead of reporposing existing convolution
hardware to support GEMM. In \autoref{chap:conv_gemm_equiv:proof}, a proof for
the equivelence between GEMM and 1x1 Convolutions is given. Additionally, in
\autoref{chap:conv_gemm_equiv:application} the afformentioned proof will be used
in tandem with the approach in \cite{cafe_con_troll} to provide support for
arbitrary convolution operations. Finally in
\autoref{chap:conv_gemm_equiv:overhead} a discussion of the overheads associated
with the approach in \autoref{chap:conv_gemm_equiv:application} is given.

\clearpage

\subsection{Functional equivelence between GEMM and 1x1 Convolutions}
\label{chap:conv_gemm_equiv:proof}

We can establish functional equivelence between GEMM and 1x1 convolutions with
the following proof. An illustration of this proof is given in
\autoref{fig:gemmTo1x1Conv}.  
Given two matricies $A\in \mathbb{R}^{Z \times C}$ and $B \in \mathbb{R}^{C
\times F}$, let $R \in \mathbb{R}^{Z\times F} = A.B$. A different way to express
the matrix multiplication $A.B$ is \autoref{math:gemm_array_notation}.  

\begin{equation}
    \begin{aligned}
        R[z][f] = \displaystyle\sum\limits_{c=0}^{C-1}A[z][c]\times B[c][f] \\ \forall z\in[0, n^2-1]
    \end{aligned}
    \label{math:gemm_array_notation}
\end{equation}

Transposing $A$ and $B$ yields $\hat{A} \in \mathbb{R}^{C\times Z}$ and $\hat{B} \in
\mathbb{R}^{F\times C}$. Using the identity $(A.B)^T = B^T.A^T$ we can rewrite
\autoref{math:gemm_array_notation} as
\autoref{math:gemm_array_notation_transpose} where $\hat{R} \in
\mathbb{R}^{F\times Z}$

\begin{equation}
    \begin{aligned}
        \hat{R}[f][z] = \displaystyle\sum\limits_{c=0}^{C-1}\hat{B}[f][c]\times \hat{A}[c][z] \\ \forall z\in[0, Z-1]
    \end{aligned}
    \label{math:gemm_array_notation_transpose}
\end{equation}

We can reshape $\hat{A}$ and $\hat{B}$ using \autoref{math:a_b_reshape} into 3D
tensors by adding an additional dimention of size 1 for $\hat{A}$ and 2
additional dimentions of size 1 for $\hat{B}$.

\begin{equation}
    \begin{aligned}
        \hat{A} \xrightarrow[]{Reshape} &\hat{A}  \in \mathbb{R}^{C \times Z\times 1}   &\hat{B} & \xrightarrow[]{Reshape} \hat{B}  \in \mathbb{R}^{F \times C\times 1\times 1} \\
    \end{aligned}
    \label{math:a_b_reshape}
\end{equation}

Applying \autoref{math:a_b_reshape} to
\autoref{math:gemm_array_notation_transpose} yields
\autoref{math:gemm_as_conv_1} where $\hat{R} \in \mathbb{R}^{F \times Z\times 1}
$ remains the transposed output of $A.B$. 

\begin{equation}
    \begin{aligned}
        \hat{R}[f][z][0] = \displaystyle\sum\limits_{c=0}^{C-1}\hat{A}[c][z][0]*\hat{B}[f][c][0][0] \\ \forall z \in [0, Z-1] 
        \end{aligned}
    \label{math:gemm_as_conv_1}
\end{equation}

Adding kernel summations to \autoref{math:gemm_as_conv_1} yields
\autoref{math:gemm_as_conv_2} which is equivelent to a 1x1 convolution of stride
1. To recover $R$ from $\hat{R}$ we can reshape $\hat{R}$ by removing the
last dimention and then transpose it. 

\begin{equation}
    \begin{aligned}
        \hat{R}[f][z][0] = \displaystyle\sum\limits_{c=0}^{C-1}\displaystyle\sum\limits_{k_x=0}^{1} \displaystyle\sum\limits_{k_y=0}^{1}\hat{A}[c][y+ky][x+kx]*\hat{B}[f][c][k_y][k_x] \\ \forall y \in [0, Z-1] \land x = 0
    \end{aligned}
    \label{math:gemm_as_conv_2}
\end{equation}

\begin{figure}[]
    \centering
    \includegraphics[scale=0.5]{fig/GemmTo1x1Conv.pdf}
    \caption{\ac{GEMM} and 1x1 Convolution Equivelence}
    \label{fig:gemmTo1x1Conv}
\end{figure}


\subsection{Supporting unsupported kernel sizes in a direct convolution accelerator}
\label{chap:conv_gemm_equiv:application}

We can support previously unsupported kernel sizes by combining the GEMM to 1x1
Conv conversion in \autoref{chap:conv_gemm_equiv:proof} with any tensor
lowering/lifting approach in \cite{cafe_con_troll}. Lowering converts a
convolution into a GEMM operation, and the approach in
\autoref{chap:conv_gemm_equiv:proof} reinterprets that operation as another 1x1
Convolution. This approach allows any convolution accelerator that can support
1x1 convolution operations to support any arbitrary convolution operation. A
visual illustration for this technique is presented in \autoref{fig:conv2gemm2conv}.
To demonstrate this approach we begin by lowering both IFmap using
\autoref{math:balanced_lowering_ifmap} and Weights using
\autoref{math:balanced_lowering_weight}. This results in two matricies IFmap and
Weights in \autoref{math:conv2gemm2conv:lowering}. Lowering should be performed
if the kernel size of the Weight tensor is unsupported $K' \notin
\{SupportedKernels\}$. 

\begin{equation}
    \begin{aligned}
        IFmap \in R^{C\times n\times n} & \xrightarrow[]{Balanced Lowering} \hat{IFmap} \in R^{nm\times K'C} \\
        Weight \in R^{F\times C\times K' \times K'} & \xrightarrow[]{Balanced Lowering} \hat{Weight} \in R^{K'C\times K'F} \\
    \end{aligned}
    \label{math:conv2gemm2conv:lowering}
\end{equation}

After lowering both tensors, we apply the transformations in
\autoref{math:conv2gemm2conv:transform} to reinterpret the anticipated GEMM
operation that occurs after lowering into a 1x1 convolution operation. The
transformations are composed of a transpose operation followed by a reshape
operation that appends additional dimentions of size 1 to both IFmap and
Weights. The transformations yields two new tensors $\hat{IFmap}$ and
$\hat{Weight}$.

\begin{equation}
    \begin{aligned}
        \hat{IFmap}^T \in R^{K'C \times nm} & \xrightarrow[]{Reshape} \hat{IFmap} \in R^{K'C \times nm \times 1} \\
        \hat{Weight}^T \in R^{K'F\times K'C} & \xrightarrow[]{Reshape} Weight \in R^{K'F\times K'C \times 1 \times 1} \\
        \end{aligned}
    \label{math:conv2gemm2conv:transform}
\end{equation}

After performing the transformations in \autoref{math:conv2gemm2conv:transform}
the output $\hat{OFmap_{prelift}}$ can be calculated after performing a 1x1
convolution in \autoref{math:conv2gemm2conv:conv} using the $\hat{IFmap}$ and
$\hat{Weight}$ tensors. 

\begin{equation}
    \begin{aligned}
        \hat{OFmap_{prelift}} \in R^{K'F\times nm \times 1} = \hat{IFmap}*\hat{Weight}
        \end{aligned}
    \label{math:conv2gemm2conv:conv}
\end{equation}

Finally we can lift $\hat{OFmap_{prelift}}$ by first reshaping it into a 2D
matrix by dropping the last dimension and then transposing it. After that, we
can apply balanced lifting in \autoref{math:balanced_lifting_ofmap} to get the
final OFmap in \autoref{math:conv2gemm2conv:lift}.

\begin{equation}
    \begin{aligned}
        \hat{OFmap_{prelift}} \in R^{K'F \times nm \times 1} \xrightarrow[]{Reshape} OFmap_{prelift} \in R^{K'F\times nm} \\
        OFmap_{prelift}^T \in R^{nm\times FK} \xrightarrow[]{Balanced Lifting} OFmap \in R^{F \times m \times m} \\
            \end{aligned}
    \label{math:conv2gemm2conv:lift}
\end{equation}

\begin{figure}[]
    \centering
    \includegraphics[scale=0.5]{fig/ConvToGemmToConv.pdf}
    \caption{Illustration of approach Conv2Gemm2Conv approach}
    \label{fig:conv2gemm2conv}
\end{figure}

\subsection{Overhead of extending support}
\label{chap:conv_gemm_equiv:overhead}

Lowering and lifting introduce additional overheads with regards to latency and
tensor sizing. The latency for performing balanced lowering and lifting is
$m^{2}K$ for a layer with a Weight tensor $\mathbb{R}^{F\times C\times K\times
K}$ and an OFmap tensor $\mathbb{R}^{F\times m\times m\times}$. While lowering
and lifting can be performed by a convolutions accelerator, in this thesis it is
assumed that a software processor on the same chip performs these operations.
Lowering also introduces duplicate data elements in the IFmap tensor thus
increasing it's overall size. In this thesis, it is assumed that on-chip memory
can account for this increase and that any additional DRAM accesses incurred
from lowering will be included in energy calculations when reporting energy
consumption of the accelerator.

To enable GEMM operations using the approach in
\autoref{chap:conv_gemm_equiv:proof} both input and output matricies are
transposed and reshaped. All reshape operations discussed in this chapter add a
dimention of size 1 to the data and they incur no data reorganization overhead.
Additionally, all transpose operations are assumed to be performed during
transfer to and from accelerator on-chip and thus incur no latency penalty. A
discussion of how transfers to and from on-chip memory can mask the latency of
transposing matricies is left as part of future work along with incorporating
lowering and lifting into the accelerator. 


\section{Layer Decomposition}


\section{Descriptor Generation}
\label{chap:sams:acc_scheduling}

We can generate descriptor programs for the memories (now referred to as SAMs)
in the final architecture template discussed in \autoref{chap:hw_dse:intro}.
These descriptor programs will be used to perform memory operations necessary
for the convolution operation to take place in the final architectural template.
Since we have two operational modes based on the discussion in
\autoref{chap:dataflow_dse:intro} 1x1/GEMM and 3x3 convolutions we will discuss the
descriptor programs required for both of these modes to take place independently of
the other. In both modes we will first highlight the memories participating in
the operation followed by the required descriptor programs for each of those
memories. Please note that muxing plays a part in coordinate the transfer of
data between different memories. The programming of muxes in tandem with SAMs is
left as part of future work. Additionally, all interactions between SAMs and
DRAM are left as part of future work. IFmap and OFmap data is assumed to be read
from and written to DRAM before and after the operation of the SAM programs
discussed in thi section. Latencies and energy penalties associated with DRAM
will be considered in \autoref{chap:hero} but SAM program generation discussed
here is DRAM agnostic.

\subsection{1x1 convolution programs}
\label{chap:sams:acc_scheduling:1x1}

To illustrate how SAMs can be programmed to perform a 1x1 convolution we will
use the final architecture in \autoref{chap:hw_dse:intro} and a 1x1 convolution
layer with 16 channels and 8 filters as a driving example. In
\autoref{chap:hw_dse:intro} there are a total of 9 processing engines with all 9
mapped to the accelerator horizontal spatial axis which enables creates an
effective an unroll factor of 9 at kernel sizes 1x1. Based on the tiling
discussion in
\autoref{chap:dataflow_dse:pruning:applying_it:loop_unroll_factors}, the
architecture tiles the weight tensor into 16 tiles (padding included) as
illustrated in \autoref{fig:1x1conv_sched_tiling}.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{fig/1x1conv_sched_tiling.pdf}
    \caption{Hardware Implementation Taxonomy adapted from \cite{maestro}}
    \label{fig:1x1conv_sched_tiling}
\end{figure}

For each weight in the tiles of \autoref{fig:1x1conv_sched_tiling} the channel
feature map corresponding to that weight has to be streamed into the PE
processing the output of that weight. For example, tile highlighted in red needs
channels C0-C8 streamed into the PEs storing those for processing. After the red
tile is processed, the green tile is loaded into the PEs weight buffer (assuming
ASAP scheduling as seen in
\autoref{chap:dataflow_dse:pruning:applying_it:loop_ordering}). Channels C9-C15
then needs to be streamed into the PEs holding the weights corresponding to
those channels. This means that channel memories may need to hold multiple
channels that are streamed out depending on the index of the tile being
processed. An illustration of how channel feature maps are stored on the channel
SAMs is present in \autoref{fig:channel_banking}. PEs holding 0 valued weights
due to padding have no corresponding channel data so nothing is streamed into
them as reflected in the 0 padding of the last channel SAM attached to the 9th
PE in \autoref{fig:channel_banking}. Additionally, they are assumed to just
forward any partial sums/ output feature maps recieved from their input to their
output with no modifications.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.495]{fig/1x1conv_ifmap_banking.pdf}
    \caption{Hardware Implementation Taxonomy adapted from \cite{maestro}}
    \label{fig:channel_banking}
\end{figure}

Since 1x1 convolutions involve entire channel feature maps streamed into PEs
with no reuse within a feature map occuring as in the 3x3 case, the only
memories that need to be programmed are the L3 channel memories and the OFmap
memories. An illustration of the required descriptor programs for the
afformentioned memories is given in \autoref{fig:1x1conv_sched}. In
\autoref{fig:1x1conv_sched} irrelevant layers and muxing as well as some PEs
have been ommited to for brevity.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.495]{fig/1x1conv_sched.pdf}
    \caption{Hardware Implementation Taxonomy adapted from \cite{maestro}}
    \label{fig:1x1conv_sched}
\end{figure}

In \autoref{fig:1x1conv_sched} channel memories can be implemented with SAMs.
For each channel SAM there are two descriptors types that appear frequently in
their programs, a wait descriptor and generate descriptor. Channel SAMs need to
perform timed reads due to the systolic delays arising from the systolic
reduction of partial sums into output feature maps. Therefore, an initial wait
instruction due to the systolic delay required by each SAM is inserted at the
begining of their descriptor programs. The delay defined by the wait descriptor
for each SAM depends on the index of the PE that the SAM is connected to. So the
first PE's corresponding SAM has no read delay so the x\_count variable in the
wait descriptor is set to 0. The next PE's channel SAM has a delay of 1 so it's
initial weight descriptor's x\_count is set to 1. After the initial wait
descriptor, each channel SAM needs to stream out a feature map. Depending on the
index of the tile being processed, each SAM streams out a different IFmap. What
distinguishes each IFmap stored on a channel SAM from another is it's start
index. For example, for the first tile highlighted in red, the first PE streams
out the IFmap begining at start index 0 with a generate descriptor. The size of
that IFmap is assumed to by $XY$ where $X$ and $Y$ are the width and height of
the IFmap. The generate descriptor increments the internal address "addr" $XY$
times with "addr" starting at 0. The corresponding generate descriptor that
manipulates the "addr" like that is a generate descriptor with an x\_count of
$XY$ and a start index of 0. When the first PE begins processing the second
tile, it reads out the IFmap stored at index $XY$ or more generally
$MOD(tile_{idx},2).XY$ since each filter has 2 tiles. This generate descriptor
is repeated for each tile in the weight tensor assuming that no padding. If a PE
is storing a 0 valued weight due to padding the generate descriptor is replaced
with a wait descriptor with an x\_count of $XY$. Optimizating descriptors to
reduce code size is left as part of future work.

For the OFmap SAM two address generators are required due to the read modify
write nature of OFmaps. The read port begins reading the layer bias immediately
and streams it into the first PE as part of partial sum reduction. All later
reads from the OFmap SAM are for partial sums that have yet to be accumulated
into OFmaps. The read descriptor required for streaming in bias/ partial sums is
a generate descriptor that streams out the contents of OFmap in a loop. It
achieves this by setting x\_count to $XY$ to stream out partial sums of size
$XY$ and y\_count to 2 which the number of tiles in a filter. To reset the
"addr" index of the read descriptor the y\_modify is set to $-XY$. This generate
descriptor is repeated 8 times where 8 is the number of filters present
processed by the PEs assuming no filter padding. Assuming ASAP scheduling as
discussed in \autoref{chap:dataflow_dse:pruning:applying_it:loop_unroll_factors}
OFmaps are written to DRAM as soon as they are completed and are not kept on
chip.

The write port waits for $C_{UNROLL}$ number of cycles to write the first
partial sums that will eventually become OFmaps once the filter being processed
concludes. IFmaps of $XY$ size less than 9 (the number of PEs in the horizontal
axis) will cause additional delay cycles to be introduced via wait descriptors
to allow partial sums to propogate through the PEs to reach the OFmap. The
reduction performed by the PEs are sequential and not combinatorial, hence the
inclusion of registers between each PE. Alleviating dead cycles due to smaller
input feature maps is left as part of future work. The descriptor required for
the write address generator are similair to the the read ones with the exception
of an additional wait descriptor that gives the first partial sum/ Ofmap time to
propogate through the systolic reduction. The delay required by the wait
descriptor is equal to the number of PEs present in the horizontal axis. After
the wait descriptor comes a generate descriptor that writes the partial sum
output/ OFmap output into the OFmap SAM in a loop. The write generatord
descriptor is repeated 8 times as well assuming no padding similair to
the read generate descriptor loop.

\subsection{3x3 convolution programs}
\label{chap:sams:acc_scheduling:3x3}