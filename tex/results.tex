\chapter{HERO Architecture Simulation}
\label{chap:results}

To assess the performance of the HERO architecture a cycle accurate simulation
enviornment was implemented using SystemC and python. The enviornment was used
to a single configurations of the HERO architecture on real networks implemented
in pytorch from the TIMM library. The simulator also enables a study of the HERO
architecture by allowing different configurations of HERO to be passed in during
simulation. This study is left as part of future work. The simulator is composed
of two parts. 1) A python based frontend that can analyse the performance
results of any pytorch network on a configuration of HERO. 2) A SystemC based
backend for cycle accurate simulations of HERO running convolution/linear layers. In
this chapter we will first discuss the simulation enviornment and it's
components in \autoref{chap:hero:sim_platform}. The results from simulating all
695 networks from TIMM using a small HERO instance are given
\autoref{chap:hero:results}.

\section{Simulation Enviornment}
\label{chap:hero:sim_platform}

\subsection{Python Frontend}
\label{chap:hero:sim_platform:frontend}

The python frontend depicted in \autoref{fig:frontend}, expects a list of pytorch models and a dictionary defining the
configuration of a HERO instance as inputs. The configuration specifies the unroll factors
for F, C, the list of kernels sizes supported in direct mode, and the on-chip
storage dedicated for input and output feature map reuse, as well as the maximum
line size in any IFmap tensor.  The output of the
frontend is a pandas dataframe containing the results from the backend
simulation of the network(s) on the specified hero instance.  
After the frontend
recieves the list of pytorch models, CIGAR extracts the dimensions of all
supported layers (Conv2D and Linear). Afterwhich two layer transformation
operations are performed. The first converts convolution layers that are
unsupported directly into equivalent (1, 1) convolutions as described in
\autoref{chap:dda:dataflow_dse:indirect_mode}. Additionally, linear layers are
reinterpreted as (1, 1) convolutions based on the proof derived from
\autoref{chap:dda:dataflow_dse:indirect_mode:conv_gemm_equiv:proof}. 
The HERO arch. config being
evaluated is used to determine what layers are supported directly
After all layers have been converted to equivelent layers
that are supported by HERO, a layer decomposition step is performed to
decompose layers that have featuremaps that are two large to fit in the
HERO's on-chip memory. 
After all layer transformations are
performed, the layers are converted to test cases and a testcase deduplication
operation is used to limit the number of simulation runs required to simulate the network(s)
on HERO. 
After the testcase queue is generated it gets passed off to several
worker threads that manage independent backend instances that simulate the different
layers of the input network(s) concurrently. Note that since we're only
concerned with the architecutre metrics produced by the simulation the layers being
simulated don't use real weights and featuremaps from a forward inference pass
of the original network. Instead equivelent layer weights and featuremaps are
instantiated in the backend to avoid unnecessary transfer of data between the
front and backend.      


\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.58]{fig/hero-sim-frontend.pdf}
    \caption{Illustration of the simulation enviornment's python frontend}
    \label{fig:frontend}
\end{figure}


\subsection{SystemC backend}
\label{chap:hero:sim_platform:backend}

The SystemC backend depicted in \autoref{fig:backend}, expects all inputs to be passed in as command line
arguments. The main inputs are 1) architecture configuration and 2) layer
configuration needed for simulation by the frontend. Sim results are generated
and sent back to the frontend using protobufs. The backend first instantiates a
HERO instance using the architecute configuration passed as input. Then it
generates an equivelent layer based on the input layer configuration. Finally
using both layer and architecture configuration it generates the SAM descriptors
required to perform all data movement operations on-chip. A dram load is then
simulated in zero time to transfer the layer data and descriptors to the on-chip
memories of the newly constructed HERO instance. Once the initialized HERO
instance is ready, the cycle accurate simulation starts and continues until all
SAMs reach a suspend descriptor. Then a dram load operation is again performed
in zero time to transfer the results from the HERO instance for validation.
After the simulated dram load, a protobuf is constructed, then populated with
the result of ouput validation and simulation results (assuming validation
success) and sent back to the frontend for further analysis and aggregation. 

The backend simulates interactions between the architecture and DRAM
functionally, in zero-time to avoid the complexity of DRAM and SAM interactions.
The interaction between SAMs and DRAM are left as part of future work. 

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.5]{fig/hero-sim-backend.pdf}
    \caption{Illustration of the simulation enviornment's SystemC backend}
    \label{fig:backend}
\end{figure}

\section{Experimental Results}
\label{chap:hero:results}

The configuration for the simulated architecture is given in
\autoref{tab:hero_config} based on the dimensions discovered in
\autoref{chap:dataflow_dse:exploring:results} and the sizing conclusions from
\autoref{chap:dataflow_dse:memory_hierarchy_sizing}. Note that other larger HERO
variants could have been chosen for this work, however, to minimize simulation
time and to explore a variant of HERO fit for more constrained embedded
enviornments the configuration in \autoref{tab:hero_config} were chosen.   
Note that sizes for on-chip
storage are given in the aggregate. The number of banks for IFmap L3 and OFmap
storage scales with the chosen $F_{unroll}$ and $C_{unroll}$ factors. As a
result, the size of individual IFmap L3 and OFmap bank is proportional with the
overall size of IFmap L3 and OFmap on-chip storage but inversely proportional
with the chosen unroll factors. The CPU baseline used in this work is an AMD
5950X processor. To minimize simulation time the frontend takes advantage of the
similarity between layer configurations accross networks in the timm library.
The total number of both convolution and linears layers post layer configuration
deduplication is reduced from 182773 to 6048. This enables a simulation time of
less than 1.5 hours for all 695 Networks.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.58]{Plots/overview/percent.pdf}
    \caption{Percent of computation represented by layers studied in the TIMM library}
    \label{fig:percent_of_compute}
\end{figure}

The layers simulated (convolution and linear) represent a substantial portion of
total network computation as show in \autoref{fig:percent_of_compute}. 
To determine percentage of compute Pytorch can be used to estimate the
total number of MAC operations used by natively supported layers. Unfortunately
many networks use custom layers constructed with pytorch primitives. For
each of these layers an analytical expression would need to be created to calculate
the number of operations in these layers. For simplicity the runtimes
of these layers on a CPU were used to estimate computational demand. In
\autoref{fig:percent_of_compute}, the horizontal access defines the average
percentage of total network runtime taken up by supported layers. The vertical
axis defines the number of networks where convolution and linear layers take up
x\% of the computation where x is defined on the horizontal axis. From
\autoref{fig:percent_of_compute} it can be seen that the convolution and linear
layers represent the majority of network runtimes in the TIMM library. 

\begin{table}[]
    \center
    \begin{tabular}{|l|l|}
    \toprule
    Config. Param. & On-Chip Storage in Bytes    \\ 
    \midrule
    Weight Storage            & 16 B / PE (8 bit precision)  \\ \hline
    IFmap L3 Storage          & ~1 MB (8 bit precision)   \\ \hline
    IFmap L2 Storage          & 512 B (8 bit precision)   \\ \hline
    OFmap Storage             & 1 MB (16 bit precision)   \\ \hline
    $C_{unroll}$              & 18   \\ \hline
    $f_{unroll}$              & 32   \\ \hline
    Directly Supported Kernels             & {(1, 1), (3, 3)}   \\ \hline
    Assumed CLK Speed             & 1 Ghz   \\ \hline
\end{tabular}
\caption{HERO configuration used for analysis}
\label{tab:hero_config}
\end{table}

\subsection{Utilization}
\label{chap:hero:results:utilization}

Utilization can be used as a surrogate for how well layers map to the selected
HERO architecture based on the dataflow optimizations discussed in
\autoref{chap:arch_dimensioning}. From figure
\autoref{fig:network_layer_level_utilization} we can see that some networks benefit substantially from the architecture while
others don't. Network level distribution of average layer utilization is generally flat with
about a third of networks not benefiting from the architecture due to low
utilization. Low utilization is defined as average PE utilization throughout a
layer's computation that's less than 50\%.  


\begin{figure}
    \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{Plots/utilization/network.pdf}}
    \hspace{0.1cm} 
    \subfigure[]{\includegraphics[width=0.48\textwidth]{Plots/utilization/layers.pdf}}
    \caption{Average layer utilization distributions by a) Network, b)Layer}
    \label{fig:network_layer_level_utilization}
\end{figure}

At the layer level we can see a more pronounced disparity in how much the
architecture benefits some layers over others. The next series of figures will
explore the cause of this disparity in utilization. If the cause of low
utilization in most layers is layer size as defined by number of operations then
utilization would generally scale with number of operations. Figure
\autoref{fig:utilization_macs_scaling} shows that the expected trend of
utilization scaling with MACS generally holds. However for a portion of the
layers on the bottom right utilization is low while MACs ops are high. This low
utilization would be especially concerning if combined with a speedup factor
$<1X$ over CPU baseline for that layer. One possible cause of low utilization,
combined with high MAC OPs and low speedup could be a high layer memory
footprint. To examine the effect of layer memory footprint as a cause for low
utilization, a scatter plot of layer feature map size vs utilization is
presented in \autoref{fig:scatter_plot_util_vs_fmap} for layers where speedup
was <1 compared to the CPU baselin. 


\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.58]{Plots/utilization/util_vs_macs.pdf}
    \caption{Shaded contour plot of average layer utilization vs number of MACs}
    \label{fig:utilization_macs_scaling}
\end{figure}

From \autoref{fig:scatter_plot_util_vs_fmap} it's clear that layer memory
footprint does not correlate with low utilization. What does correlate low
utilization is layer type. The majority of low utilization layers are depthwise
convolution layers as in figure \autoref{fig:cause_of_low_util}.a. Depthwise
layers were not considered during dataflow DSE in \autoref{chap:dda}. Since they
weren't an optimization target the frontend decomposes depthwise layers into
individual groups of convolutions and since depthwise layers are convolution
layers where each IFmap channel is convolved with a kernel independently of the
others, all PEs dedicated to concurrent channel processing are underutilized.
From \autoref{fig:cause_of_low_util}.a there a few non grouped convolution
layers with low low utilization and low speedup. These layers when examined more
closely in \autoref{fig:cause_of_low_util}.b and they appear to have lower
available channel/ filter concurrency as defined by the number of channels/
filters in a layer relative to the available effective channel/ filter
concurrency of the architecture. In both cases of depthwise/grouped convolutions
and low channel/filter count non-grouped convolutions low utilization and low
speedup can be remedied by exploiting other forms of concurrency in the
architecture other that Filter/Channel/Kernel concurrency chosen by HERO. 

\begin{figure}
    \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{Plots/utilization/util_vs_ifmap.pdf}}
    \hspace{0.1cm} 
    \subfigure[]{\includegraphics[width=0.48\textwidth]{Plots/utilization/util_vs_ofmap.pdf}}
    \caption{Scatter plot of utilization vs layer a) IFmap size and b) OFmap size. The dotted blue line represents the on-chip memory allocated for both types of feature maps.}
    \label{fig:scatter_plot_util_vs_fmap}
\end{figure}

\begin{figure}
    \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{Plots/utilization/type_of_low_util.pdf}}
    \hspace{0.1cm} 
    \subfigure[]{\includegraphics[width=0.48\textwidth]{Plots/utilization/low_util_big_fmap.pdf}}
    \caption{Illustration of the causes of low utilization due to A) Layer type and B) Layer dimensions}
    \label{fig:cause_of_low_util}
\end{figure}

Paradoxically, \autoref{fig:scatter_plot_util_vs_fmap} shows a cluster with
layers with both high utilization and low speedup relative to the CPU baseline.
When examining the Channel and Filter counts for these layers in
\autoref{fig:mac_ratios}.a it can be seen that they are generally multiple orders
of magnitude greater than the available effective channel/ filter concurrency.
In effect these layers suffer from the opposite problem of the previous layers.
The resources allocation defined in \autoref{tab:hero_config} is too low to
satisfy the available filter and channel concurrency in these high utilization
but low speedup layer. 
The impact of lowering and lifting on increasing the number of MAC operations of
a layer cannot be understated. When comparing the CPU baseline vs HERO, the CPU
baseline has the inherent advantage of a larger and more complex memory
hierarchy that enables more complicated access patterns to occur. This advantage
allows CPUs to support arbitrary layers without relying on lowering/ lifting
data transformations which can dramatically increase the effective number of MAC
operations in a layer as seen in \autoref{fig:mac_ratios}.b where the median MAC
factor increase as a result of the balanced lowering/ lifting scheme chosen by
HERO is ~3.8X over the original number of MACs present in the layer. In short
HERO has to perform more operations for the same layers relative to CPU. To
remedy this issue, more layers need to be supported directly by HERO.  

\begin{figure}
    \centering
    \includegraphics[scale=0.48]{Plots/utilization/ratios.pdf}
    \hspace{0.1cm} 
    \includegraphics[scale=0.48]{Plots/utilization/macs.pdf}
    \caption{A) Barplot of the ratios between layer channel/ filters vs effective available channel/ filter concurrency for layers with speedup $<$ 1 and utilization $>$ 95\% b) Barplot of the MAC factor increase due to lowering/lifting for layers with speedup $<$ 1 and utilization $>$ 95\%}
    \label{fig:mac_ratios}
\end{figure}

\subsection{Latency and speedup over CPU Baseline}
\label{chap:hero:results:latency}

The majority of the 695 Networks in the Timm library experience a speedup over
CPU baseline on HERO with mean speedup being 6.8X over CPU baseline as show in
\autoref{fig:speedup}.a. This network level speedup is limited by layers that do
not benefit from running HERO due to poor mapping to the architecture as
discussed in \autoref{chap:hero:results:utilization}. In fact close to 12\% of
layers experience a $<$1X speed relative to CPU baseline as see in
\autoref{fig:speedup}.b. Offloading these poorly supported layers to the CPU
yields \autoref{fig:speedup}.c where the mean speedup over CPU baseline is ~30X.
Note that these results are restricted to not only the simulated config in
\autoref{tab:hero_config} but the AMD 5950X cpu used for comparison. The 5950X
is a workstation CPU with a TDP greatly in excess of what's expected in the small
embedded environments the simulated configuration in \autoref{tab:hero_config}
of HERO is expected to run in.

\begin{figure}
    \centering
    \includegraphics[scale=0.46]{Plots/latency/net_speedup.pdf}
    \hspace{0.1cm} 
    \includegraphics[scale=0.46]{Plots/latency/layer_speedup.pdf}
    \includegraphics[scale=0.46]{Plots/latency/net_speed_w_offload.pdf}
    \hspace{0.1cm} 
    \includegraphics[scale=0.46]{Plots/latency/fps.pdf}
    \caption{A) Histogram of average network speedup over CPU baseline B) Barplot of }
    \label{fig:speedup}
\end{figure}

\autoref{fig:speedup}.d  shows the FPS for layers supported by HERO. The median FPS
of supported layers is ~91 FPS. Since HERO's support is restricted to
convolution and linear operations these FPS results should serve only as an
upperbound estimate for the actual FPS when running these networks on HERO. An
exploration of CPU offloading of other layers such as activation and batchnorm
is left as part of future work.  

\subsection{DRAM Bandwidth}
\label{chap:hero:sim_platform:cigar_side}

network and layer bandwidth histograms
within ddr4 spec? cite here

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.58]{Plots/resources/net_bw.pdf}
    \caption{Hardware Implementation Taxonomy adapted from \cite{maestro}}
    \label{fig:dram_bw}
\end{figure}


\subsection{Energy}
\label{chap:hero:sim_platform:cigar_side}

used energy model in cite energy model
add energy model as table

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.58]{Plots/energy/barplot.pdf}
    \caption{Hardware Implementation Taxonomy adapted from \cite{maestro}}
    \label{fig:hw_taxonomy}
\end{figure}

mac is basically free
dram dominates

network inferences/ j median is around 57

excluding dram number jumps to 20K
there's room to reduce on-chip data movement by exploiting different kinds of
concurrency

\begin{figure}
    \centering
    \includegraphics[scale=0.48]{Plots/energy/inferences.pdf}
    \hspace{0.1cm} 
    \includegraphics[scale=0.48]{Plots/energy/inferences_wo_dram.pdf}
    \caption{A) Barplot of the ratios between layer channel/ filters vs effective available channel/ filter concurrency for layers with speedup $<$ 1 and utilization $>$ 95\% b) Barplot of the MAC factor increase due to lowering/lifting for layers with speedup $<$ 1 and utilization $>$ 95\%}
    \label{fig:speedup}
\end{figure}

\subsection{Area}

\label{chap:conv_gemm_equiv:overhead}

area model as table
bulk goes to ofmap storage and ifmap storage
ofmap storage requires higher precision

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.58]{Plots/resources/area.pdf}
    \caption{Hardware Implementation Taxonomy adapted from \cite{maestro}}
    \label{fig:hw_taxonomy}
\end{figure}

% \subsection{Descriptor program scaling}
% \label{chap:hero:sim_platform:cigar_side}

% median required storage for descriptors per address generator is around 100
% scaling can be improved with more complicated descriptors

% \begin{figure}[ht]
%     \centering
%     \includegraphics[scale=0.58]{Plots/resources/descriptors.pdf}
%     \caption{Hardware Implementation Taxonomy adapted from \cite{maestro}}
%     \label{fig:hw_taxonomy}
% \end{figure}

\subsection{Per network results}
\label{chap:hero:sim_platform:cigar_side}

networks chosen
resnet50 (popular and backbone for alot of other networks)
mobilenetv3 specific to edge devices and has depthwise layers

\subsubsection{ResNet50}

\clearpage
\begin{center}
    \begin{tabular}{lrlrrlllll}
        \toprule
        {} &  Count & IFmap &     Cin &    Cout &       K &  Stride & Padding & Groups &   Bias \\
        Name     &        &             &         &         &         &         &         &        &        \\
        \midrule
        conv\_0   & 1 &  (224, 224) &     3.0 &    64.0 &  (7, 7) &  (2, 2) &  (3, 3) &    1.0 &  False \\
        conv\_1   & 1 &    (56, 56) &    64.0 &    64.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_2   & 3 &    (56, 56) &    64.0 &    64.0 &  (3, 3) &  (1, 1) &  (1, 1) &    1.0 &  False \\
        conv\_3   & 4 &    (56, 56) &    64.0 &   256.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_4   & 2 &    (56, 56) &   256.0 &    64.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_5   & 1 &    (56, 56) &   256.0 &   128.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_6   & 1 &    (56, 56) &   128.0 &   128.0 &  (3, 3) &  (2, 2) &  (1, 1) &    1.0 &  False \\
        conv\_7   & 4 &    (28, 28) &   128.0 &   512.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_8   & 1 &    (56, 56) &   256.0 &   512.0 &  (1, 1) &  (2, 2) &  (0, 0) &    1.0 &  False \\
        conv\_9   & 3 &    (28, 28) &   512.0 &   128.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_10  & 3 &    (28, 28) &   128.0 &   128.0 &  (3, 3) &  (1, 1) &  (1, 1) &    1.0 &  False \\
        conv\_11  & 1 &    (28, 28) &   512.0 &   256.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_12  & 1 &    (28, 28) &   256.0 &   256.0 &  (3, 3) &  (2, 2) &  (1, 1) &    1.0 &  False \\
        conv\_13  & 6 &    (14, 14) &   256.0 &  1024.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_14  & 1 &    (28, 28) &   512.0 &  1024.0 &  (1, 1) &  (2, 2) &  (0, 0) &    1.0 &  False \\
        conv\_15  & 5 &    (14, 14) &  1024.0 &   256.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_16  & 5 &    (14, 14) &   256.0 &   256.0 &  (3, 3) &  (1, 1) &  (1, 1) &    1.0 &  False \\
        conv\_17  & 1 &    (14, 14) &  1024.0 &   512.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_18  & 1 &    (14, 14) &   512.0 &   512.0 &  (3, 3) &  (2, 2) &  (1, 1) &    1.0 &  False \\
        conv\_19  & 3 &      (7, 7) &   512.0 &  2048.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_20  & 1 &    (14, 14) &  1024.0 &  2048.0 &  (1, 1) &  (2, 2) &  (0, 0) &    1.0 &  False \\
        conv\_21  & 2 &      (7, 7) &  2048.0 &   512.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_22  & 2 &      (7, 7) &   512.0 &   512.0 &  (3, 3) &  (1, 1) &  (1, 1) &    1.0 &  False \\
        linear\_0 & 1 &      (1, 1) &  2048.0 &  1000.0 &     N/A &     N/A &     N/A &    N/A &   True \\
        \bottomrule
        \end{tabular}
\end{center}

\subsubsection{MobilenetV3}
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{Plots/networks/resnet50.pdf}
    \caption{Hardware Implementation Taxonomy adapted from \cite{maestro}}
    \label{fig:hw_taxonomy}
\end{figure}

\clearpage
% \Rotatebox{90}{%
\begin{center}
    \begin{tabular}{lrlrrlllll}
        \toprule
        {} &  Count &       IFmap &    Cin &    Cout &       K &  Stride & Padding & Groups &   Bias \\
        Name    &        &             &        &         &         &         &         &        &        \\
        \midrule
        conv\_0  &      1 &  (224, 224) &    3.0 &    16.0 &  (3, 3) &  (2, 2) &  (1, 1) &    1.0 &  False \\
        conv\_1  &      1 &  (112, 112) &   16.0 &    16.0 &  (3, 3) &  (2, 2) &  (1, 1) &   16.0 &  False \\
        conv\_2  &      1 &      (1, 1) &   16.0 &     8.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_3  &      1 &      (1, 1) &    8.0 &    16.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_4  &      1 &    (56, 56) &   16.0 &    16.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_5  &      1 &    (56, 56) &   16.0 &    72.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_6  &      1 &    (56, 56) &   72.0 &    72.0 &  (3, 3) &  (2, 2) &  (1, 1) &   72.0 &  False \\
        conv\_7  &      1 &    (28, 28) &   72.0 &    24.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_8  &      1 &    (28, 28) &   24.0 &    88.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_9  &      1 &    (28, 28) &   88.0 &    88.0 &  (3, 3) &  (1, 1) &  (1, 1) &   88.0 &  False \\
        conv\_10 &      1 &    (28, 28) &   88.0 &    24.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_11 &      1 &    (28, 28) &   24.0 &    96.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_12 &      1 &    (28, 28) &   96.0 &    96.0 &  (5, 5) &  (2, 2) &  (2, 2) &   96.0 &  False \\
        conv\_13 &      2 &      (1, 1) &   96.0 &    24.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_14 &      2 &      (1, 1) &   24.0 &    96.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_15 &      1 &    (14, 14) &   96.0 &    32.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_16 &      2 &    (14, 14) &   32.0 &   192.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_17 &      2 &    (14, 14) &  192.0 &   192.0 &  (5, 5) &  (1, 1) &  (2, 2) &  192.0 &  False \\
        conv\_18 &      2 &      (1, 1) &  192.0 &    48.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_19 &      2 &      (1, 1) &   48.0 &   192.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_20 &      2 &    (14, 14) &  192.0 &    32.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_21 &      1 &    (14, 14) &   32.0 &    96.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_22 &      1 &    (14, 14) &   96.0 &    96.0 &  (5, 5) &  (1, 1) &  (2, 2) &   96.0 &  False \\
        conv\_23 &      1 &    (14, 14) &   96.0 &    40.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_24 &      1 &    (14, 14) &   40.0 &   120.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_25 &      1 &    (14, 14) &  120.0 &   120.0 &  (5, 5) &  (1, 1) &  (2, 2) &  120.0 &  False \\
        conv\_26 &      1 &      (1, 1) &  120.0 &    32.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_27 &      1 &      (1, 1) &   32.0 &   120.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_28 &      1 &    (14, 14) &  120.0 &    40.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_29 &      1 &    (14, 14) &   40.0 &   240.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_30 &      1 &    (14, 14) &  240.0 &   240.0 &  (5, 5) &  (2, 2) &  (2, 2) &  240.0 &  False \\
        conv\_31 &      1 &      (1, 1) &  240.0 &    64.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_32 &      1 &      (1, 1) &   64.0 &   240.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_33 &      1 &      (7, 7) &  240.0 &    72.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_34 &      3 &      (7, 7) &   72.0 &   432.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_35 &      2 &      (7, 7) &  432.0 &   432.0 &  (5, 5) &  (1, 1) &  (2, 2) &  432.0 &  False \\
        conv\_36 &      2 &      (1, 1) &  432.0 &   112.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_37 &      2 &      (1, 1) &  112.0 &   432.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        conv\_38 &      2 &      (7, 7) &  432.0 &    72.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &  False \\
        conv\_39 &      1 &      (1, 1) &  432.0 &  1024.0 &  (1, 1) &  (1, 1) &  (0, 0) &    1.0 &   True \\
        \bottomrule
        \end{tabular}
\end{center}


\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{Plots/networks/mobilenetv3_small_075.pdf}
    \caption{Hardware Implementation Taxonomy adapted from \cite{maestro}}
    \label{fig:hw_taxonomy}
\end{figure}
